import asyncio
import json
from langchain_openai import ChatOpenAI
from ..utils import env_loader # Keep env_loader for fallback
from typing import List, Dict, Any, Optional

async def evaluate_responses(
    results: List[Dict[str, Any]],
    config: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    """
    Evaluates the responses generated by the executor using an evaluator LLM.

    Args:
        results: A list of result dictionaries from the executor.
        config: An optional dictionary containing configuration like:
            eval_model: The evaluator model name.
            eval_api_key: The API key for the evaluator model.
            eval_endpoint_url: The endpoint URL for the evaluator model.
            eval_batch_size: Max concurrent evaluation requests.
            benchmark_api_key: Fallback API key if eval_api_key is missing.
            benchmark_endpoint_url: Fallback endpoint if eval_endpoint_url is missing.

    Returns:
        A list of result dictionaries, augmented with evaluation scores and reasoning.
    """
    cfg = config or {}
    # Use config values or fallback to environment variables
    eval_model = cfg.get("eval_model", env_loader.get_env("EVAL_MODEL", "gpt-4"))
    eval_api_key = cfg.get("eval_api_key", env_loader.get_env("EVAL_API_KEY", env_loader.get_env("BENCHMARK_API_KEY")))
    eval_endpoint_url = cfg.get("eval_endpoint_url", env_loader.get_env("EVAL_ENDPOINT_URL", env_loader.get_env("BENCHMARK_ENDPOINT_URL")))
    eval_batch_size = int(cfg.get("eval_batch_size", env_loader.get_env("EVAL_BATCH_SIZE", 5)))

    if not eval_api_key:
        print("Warning: Evaluator API key not found in config or environment variables (EVAL_API_KEY, BENCHMARK_API_KEY). Evaluation might fail.")
    if not eval_endpoint_url:
         print("Warning: Evaluator endpoint URL not found in config or environment variables (EVAL_ENDPOINT_URL, BENCHMARK_ENDPOINT_URL). Evaluation might fail.")
         # Allow proceeding without endpoint if user intends to use default OpenAI endpoint

    try:
        evaluator = ChatOpenAI(
            model_name=eval_model,
            openai_api_key=eval_api_key,
            base_url=eval_endpoint_url # Can be None if using default OpenAI
        )
    except Exception as e:
        print(f"Error initializing evaluator LLM: {e}")
        # Return results without evaluation if evaluator fails to initialize
        return [ {**r, "score": None, "scoreReasoning": None, "eval_error": f"Evaluator Initialization Error: {e}"} for r in results]


    semaphore = asyncio.Semaphore(eval_batch_size)

    async def _evaluate(item):
        # Extract relevant info from the result item
        prompt_content = item.get("user_message", item.get("prompt", "")) # Handle both keys
        expected_answer = item.get("expected")
        actual_answer = item.get("actual")
        item_id = item.get("id")

        # Handle cases where the execution failed or actual answer is missing
        # Also skip evaluation if there's already an error from the executor
        if actual_answer is None or item.get("error"):
             return {
                 **item, # Keep original fields
                 "id": item_id,
                 "prompt": prompt_content,
                 "expected": expected_answer,
                 "actual": actual_answer, # Keep actual as None if it was None
                 "score": 0.0 if actual_answer is None else None, # Assign 0 only if execution failed, None otherwise for eval skip
                 "scoreReasoning": "Evaluation skipped: No actual answer generated or execution error occurred." if actual_answer is None or item.get("error") else None,
                 "eval_error": None # No *evaluation* error here
             }

        # Build evaluation prompt dynamically based on presence of expected answer
        try:
            if expected_answer:
                content = (
                    f"Reference answer:\n{expected_answer}\n\n"
                    f"Actual answer:\n{actual_answer}\n\n"
                    "Your task is to act as a strict evaluator comparing the actual answer to the reference answer.\n"
                    "Context: This might be part of a conversation.\n"
                    "✅ Focus on semantic similarity, not exact match.\n"
                    "✅ Variable values, numbers, or identifiers may vary, as long as meaning is preserved.\n"
                    "✅ Check if the actual answer delivers the same intent, logic, and meaning as the reference.\n"
                    "✅ Ignore formatting differences or phrasing variations.\n"
                    "❌ Deduct points if meaning is lost, logic is incorrect, or important elements are missing.\n\n"
                    "Respond with:\n"
                    "1. A **single numeric score between 0 and 1**, with up to two decimal places.\n"
                    "2. Followed by a short reason for this score, in **one sentence**.\n\n"
                    "Format your response exactly like this:\n"
                    "0.85\nReason: The actual answer matches the intent but misses some details.\n\n"
                    "Important: Do not add any other explanation outside this format."
                )
            else:
                # Self-evaluation if no expected answer is provided
                content = (
                    f"Actual answer:\n{actual_answer}\n\n"
                    "There is no reference answer provided.\n"
                    "Your task is to self-evaluate this answer for correctness, completeness, and clarity.\n"
                    "Context: This might be part of a conversation.\n"
                    "✅ Consider if the answer is logically sound and factually correct.\n"
                    "✅ Check if it fully answers the implied question or task.\n"
                    "✅ Reward answers that are well-structured, clear, and comprehensive.\n"
                    "❌ Deduct points for incomplete, vague, or factually incorrect responses.\n\n"
                    "Respond with:\n"
                    "1. A **single numeric score between 0 and 1**, with up to two decimal places.\n"
                    "2. Followed by a short reason for this score, in **one sentence**.\n\n"
                    "Format your response exactly like this:\n"
                    "0.90\nReason: The answer is clear, well-explained, and complete.\n\n"
                    "Important: Do not add any other explanation outside this format."
                )

            eval_prompt_messages = [
                {"role": "system", "content": "You are a strict evaluator of answers."},
                {"role": "user", "content": content}
            ]
        except Exception as prompt_build_e:
             # Error building the evaluation prompt itself
             return {
                **item,
                "score": None,
                "scoreReasoning": "Evaluation skipped: Error building evaluation prompt.",
                "eval_error": f"Prompt build error: {prompt_build_e}"
            }

        async with semaphore:
            try:
                # Use the initialized evaluator instance
                res = await evaluator.ainvoke(eval_prompt_messages)
                raw_response = res.content.strip() if hasattr(res, 'content') else str(res).strip() # Handle different response types

                score = None
                reason = "Evaluation parsing failed: Could not extract score/reason."
                try:
                    lines = raw_response.split("\n", 1)
                    if lines and len(lines) >= 1:
                        score_str = lines[0].strip()
                        # More robust score parsing
                        score_val = float(score_str)
                        score = min(1.0, max(0.0, score_val)) # Clamp score
                        if len(lines) > 1:
                             reason_prefix = "Reason:"
                             reason_line = lines[1].strip()
                             if reason_line.startswith(reason_prefix):
                                 reason = reason_line[len(reason_prefix):].strip()
                             else:
                                 reason = reason_line # Use the whole line if "Reason:" prefix is missing
                        else:
                            reason = "Score found, but reason missing in evaluator response."
                    else:
                         # If splitting fails, maybe the whole response is the score?
                         try:
                             score_val = float(raw_response)
                             score = min(1.0, max(0.0, score_val))
                             reason = "Score found, but reason missing in evaluator response."
                         except ValueError:
                             # If it's not a float either, parsing failed
                             reason = f"Evaluation parsing failed: Unexpected format '{raw_response}'"

                except (ValueError, IndexError) as parse_e:
                    print(f"Error parsing evaluation response for item {item_id}: {parse_e}, raw_response: {raw_response}")
                    # Keep score as None, update reason
                    reason = f"Evaluation parsing failed: {parse_e}. Raw: '{raw_response}'"


                # Merge evaluation results with original item data
                return {
                    **item,
                    "score": score, # Score might be None if parsing failed
                    "scoreReasoning": reason.strip(),
                    "eval_error": None,
                }

            except Exception as eval_e:
                print(f"Evaluation Exception for item {item_id}: {eval_e}")
                return {
                    **item,
                    "score": None, # Indicate evaluation failed
                    "scoreReasoning": None,
                    "eval_error": f"Evaluation API Error: {str(eval_e)}",
                }

    # Create tasks for evaluation
    evaluation_tasks = [_evaluate(r) for r in results]
    evaluated_results = await asyncio.gather(*evaluation_tasks)

    return evaluated_results
